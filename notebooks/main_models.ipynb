{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropping columns and splitting data into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = 'All' \n",
    "# model_config = 'Diff'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/final_data.csv')\n",
    "\n",
    "metrics = ['FT{}G', '{}S', '{}ST', '{}Acc', '{}Conv']\n",
    "team_metrics = ['{}Value', '{}CurrentPoints', '{}Rating']\n",
    "refs = ['H', 'A']\n",
    "\n",
    "stats_cols = [m.format(ref) for m, ref in [(m, r) for m in metrics for r in refs]]\n",
    "team_cols = [m.format(ref) + '_5Avg' for m, ref in [(m, r) for m in metrics for r in refs]] + [m.format(ref) for m, ref in [(m, r) for m in team_metrics for r in refs]]\n",
    "diff_cols = [m.format('') + '_5Avg_Diff' for m in metrics] + [m.format('') +'_Diff' for m in team_metrics]\n",
    "other_cols = ['Season', 'Date', 'HomeTeam', 'AwayTeam']\n",
    "\n",
    "dropped_columns = ['FTR'] + other_cols + stats_cols\n",
    "if model_config == 'All':  dropped_columns += diff_cols\n",
    "elif model_config == 'Diff': dropped_columns += team_cols \n",
    "    \n",
    "df_train = df[df.Season <= '2017-2018']\n",
    "df_train_x = df_train.drop(columns = dropped_columns)\n",
    "df_train_y = df_train.FTR\n",
    "\n",
    "df_test = df[df.Season == '2018-2019']\n",
    "df_test_x = df_test.drop(columns = dropped_columns)\n",
    "df_test_y = df_test.FTR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, df_train_x, df_train_y, df_test_x, df_test_y):\n",
    "    train_predictions = model.predict(df_train_x)\n",
    "    train_accuracy = (train_predictions == df_train_y).mean()\n",
    "    print('Training Accuracy = {:0.3f}%'.format(train_accuracy))\n",
    "    test_predictions = model.predict(df_test_x)\n",
    "    test_accuracy = (test_predictions == df_test_y).mean()\n",
    "    print('Testing Accuracy = {:0.3f}%'.format(test_accuracy))\n",
    "    \n",
    "def join_pred_on_data(model, df_test):\n",
    "    df_test_pred = df_test.copy()\n",
    "    classes = model.classes_\n",
    "    preds_proba = model.predict_proba(df_test_x)\n",
    "    preds = model.predict(df_test_x)\n",
    "    \n",
    "    for i, cl in enumerate(classes):\n",
    "        df_test_pred['PRED_{}'.format(cl)] = np.vectorize(lambda x: round(x, 3))(preds_proba[:, i])\n",
    "    \n",
    "    df_test_pred['PRED_RESULT'] = preds\n",
    "    df_test_pred = df_test_pred[[x for x in df_test_pred.columns if x != 'FTR'] + ['FTR']]\n",
    "        \n",
    "    return df_test_pred\n",
    "\n",
    "param_grid = {\n",
    "    'bootstrap': [True],\n",
    "    'max_depth': [50, 70, 90],\n",
    "    'max_features': [2, 3, 4],\n",
    "    'min_samples_leaf': [5, 7],\n",
    "    'min_samples_split': [10, 12],\n",
    "    'n_estimators': [20, 50, 500, 1500]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training model with Cross validation on training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_train = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 144 candidates, totalling 432 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  80 tasks      | elapsed:   36.7s\n",
      "[Parallel(n_jobs=-1)]: Done 230 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=-1)]: Done 432 out of 432 | elapsed:  3.7min finished\n"
     ]
    }
   ],
   "source": [
    "if model_config == 'All': model_file = '../saved_models/RF_all.sav'\n",
    "elif model_config == 'Diff': model_file = '../saved_models/RF_diff.sav'\n",
    "\n",
    "if re_train:\n",
    "    rf = RandomForestClassifier()\n",
    "    grid_search = GridSearchCV(estimator = rf, param_grid = param_grid, cv = 3, n_jobs = -1, verbose = 1)\n",
    "    grid_search.fit(df_train_x, df_train_y)\n",
    "    best_rf = grid_search.best_estimator_\n",
    "    \n",
    "else:\n",
    "    best_rf = pickle.load(model_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assessing performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy = 0.768%\n",
      "Testing Accuracy = 0.594%\n"
     ]
    }
   ],
   "source": [
    "evaluate(best_rf, df_train_x, df_train_y, df_test_x, df_test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(best_rf, open(model_file, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_test_pred = join_pred_on_data(best_rf, df_test)\n",
    "# df_test_pred.iloc[104]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
